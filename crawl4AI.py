import json
import os
import re
import unicodedata
from urllib.parse import urlparse

from crawl4ai import AsyncWebCrawler, CacheMode
from bs4 import BeautifulSoup
import google.generativeai as genai
from datetime import datetime
from metadata_helper import extract_metadata_from_content

# Load environment variables

# ====== CONFIG ======
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyBEmmuMA1dNa28-r7UWuTxWtUvHvCy5j34")
genai.configure(api_key=GEMINI_API_KEY)

MODEL_NAME = "models/gemini-2.5-flash"

# --- L·ªçc r√°c (t·ªëi thi·ªÉu, d·ªÖ m·ªü r·ªông) ---
JUNK_PATTERNS = [
    r"l∆∞·ª£t xem",
    r"views today",
    r"view today",
    r"xem th√™m",
    r"ƒë·ªçc th√™m",
    r"kh√°m ph√° ngay",
    r"s·ªë l∆∞·ª£ng ng∆∞·ªùi",
    r"ƒë√£ xem b√†i vi·∫øt",
]


def _normalize_ws(text: str) -> str:
    """Chu·∫©n ho√° kho·∫£ng tr·∫Øng."""
    text = re.sub(r"\s+", " ", (text or "")).strip()
    return text


def _remove_junk(text: str) -> str:
    """B·ªè c√¢u r√°c ki·ªÉu CTA / view count."""
    if not text:
        return ""
    chunks = re.split(r"[\n\r]+", text)
    keep: list[str] = []
    for c in chunks:
        s = _normalize_ws(c)
        if len(s) < 15:
            continue
        low = s.lower()
        if any(re.search(p, low) for p in JUNK_PATTERNS):
            continue
        keep.append(s)
    return " ".join(keep).strip()


def _truncate_words(text: str, min_words: int, max_words: int) -> str:
    """C·∫Øt theo s·ªë t·ª´ (kh√¥ng b·ªãa th√™m n·∫øu thi·∫øu)."""
    text = _normalize_ws(_remove_junk(text))
    if not text:
        return ""
    words = text.split()
    if len(words) <= max_words:
        return " ".join(words)
    return " ".join(words[:max_words])


def _slugify(text: str) -> str:
    """T·∫°o slug an to√†n (kh√¥ng d·∫•u) ƒë·ªÉ l√†m id."""
    text = (text or "").strip().lower()
    if not text:
        return "unknown"
    text = unicodedata.normalize("NFKD", text)
    text = "".join(ch for ch in text if not unicodedata.combining(ch))
    text = re.sub(r"[^a-z0-9]+", "_", text)
    text = re.sub(r"_+", "_", text).strip("_")
    return text or "unknown"


def _hub_id_from_url_or_name(url: str, name: str) -> str:
    """∆Øu ti√™n slug t·ª´ URL, fallback sang title."""
    try:
        path = urlparse(url).path.strip("/")
        segs = [s for s in path.split("/") if s]
        # l·∫•y segment cu·ªëi n·∫øu c√≥, b·ªè .html
        cand = segs[-1] if segs else ""
        cand = re.sub(r"\.html?$", "", cand, flags=re.IGNORECASE)
        cand = cand.replace("-", " ")
        slug = _slugify(cand)
        if slug and slug != "unknown":
            return slug
    except Exception:
        pass
    return _slugify(name)


def _find_main_content(soup: BeautifulSoup):
    """Ch·ªçn v√πng content ch√≠nh, tr√°nh nav/footer/sidebar."""
    for t in soup(["script", "style", "noscript", "svg", "iframe"]):
        t.decompose()

    candidates = [
        soup.find("main"),
        soup.find("article"),
        soup.find("div", id="content"),
        soup.find("div", class_="content"),
        soup.find("div", class_="entry-content"),
        soup.find("div", class_="post-content"),
    ]
    for c in candidates:
        if c and len(c.get_text(" ", strip=True)) > 400:
            return c

    body = soup.body or soup
    for t in body.find_all(["header", "footer", "nav", "aside"], recursive=True):
        t.decompose()
    return body


def _text_from_node(tag) -> str:
    """L·∫•y text s·∫°ch t·ª´ node; gi·ªØ bullet cho ul/ol."""
    if not tag:
        return ""

    if tag.name in ("ul", "ol"):
        items = []
        for li in tag.find_all("li", recursive=False):
            s = _normalize_ws(li.get_text(" ", strip=True))
            s = _remove_junk(s)
            if len(s) >= 10:
                items.append(f"- {s}")
        return "\n".join(items)

    # p/div/span/strong...
    s = _normalize_ws(tag.get_text(" ", strip=True))
    return _remove_junk(s)


def _collect_section_text(heading_tag, max_words: int) -> str:
    """Gom text sau heading cho t·ªõi heading k·∫ø ti·∫øp; c√≥ th·ªÉ l·∫•y ul/li/strong."""
    parts: list[str] = []

    # kh√¥ng nh√©t heading v√†o content (tr√°nh d√≠nh ti√™u ƒë·ªÅ)
    for sib in heading_tag.find_next_siblings():
        if sib.name in ("h1", "h2", "h3", "h4", "h5"):
            break

        if sib.name in ("p", "div", "ul", "ol"):
            t = _text_from_node(sib)
            if t:
                parts.append(t)

        # stop theo word-count
        if sum(len(p.split()) for p in parts) >= max_words:
            break

    return _normalize_ws("\n".join(parts))


def build_heading_keywords_prompt(html_sample: str) -> str:
    """
    Prompt ƒë·ªÉ LLM t√¨m heading keywords (LIGHTWEIGHT)
    """
    return f"""Analyze this HTML and identify heading keywords for each section.

Find Vietnamese keywords in headings (h2, h3) for these sections:
- overview/introduction
- weather/best time
- transportation  
- food/cuisine
- places/attractions
- travel tips

Return ONLY JSON:
{{
  "keywords": {{
    "weather": ["th·ªùi ti·∫øt", "kh√≠ h·∫≠u", "m√πa"],
    "transportation": ["ph∆∞∆°ng ti·ªán", "di chuy·ªÉn", "ƒëi l·∫°i"],
    "food": ["·∫©m th·ª±c", "m√≥n ƒÉn", "nh√† h√†ng"],
    "places": ["ƒë·ªãa ƒëi·ªÉm", "tham quan", "danh lam"],
    "tips": ["kinh nghi·ªám", "l∆∞u √Ω", "tips"]
  }}
}}

HTML sample:
{html_sample[:3000]}

Return ONLY the JSON."""


async def llm_get_heading_keywords(html_content: str) -> dict:
    """
    LLM ch·ªâ t√¨m keywords, KH√îNG extract content
    """
    model = genai.GenerativeModel(MODEL_NAME)
    prompt = build_heading_keywords_prompt(html_content)

    print("ü§ñ LLM ƒëang t√¨m heading keywords...")

    try:
        response = model.generate_content(prompt)
        text = response.text.strip()

        # Clean markdown
        if text.startswith("```json"):
            text = text[7:-3].strip()
        elif text.startswith("```"):
            text = text[3:-3].strip()

        keywords = json.loads(text)
        print("‚úÖ Keywords ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y!")
        return keywords.get("keywords", {})

    except Exception as e:
        print(f"‚ö†Ô∏è LLM failed: {e}, using default keywords")
        return {
            "weather": ["th·ªùi ti·∫øt", "kh√≠ h·∫≠u", "th·ªùi gian", "m√πa"],
            "transportation": ["ph∆∞∆°ng ti·ªán", "di chuy·ªÉn", "ƒëi l·∫°i", "xe"],
            "food": ["·∫©m th·ª±c", "m√≥n ƒÉn", "nh√† h√†ng", "ƒÉn u·ªëng"],
            "places": ["ƒë·ªãa ƒëi·ªÉm", "tham quan", "danh lam", "du l·ªãch"],
            "tips": ["kinh nghi·ªám", "l∆∞u √Ω", "tips", "m·∫πo"]
        }


def extract_content_with_beautifulsoup(html: str, keywords: dict) -> dict:
    """Extract content theo heading keywords + rule-based (kh√¥ng d√πng LLM)."""
    print("\ud83d\udd0d BeautifulSoup ƒëang extract content...")

    soup = BeautifulSoup(html, "html.parser")
    content_area = _find_main_content(soup)

    extracted = {
        "name": "",
        "overview": "",
        "location_and_geography": "",
        "weather_and_best_time": "",
        "transportation": "",
        "culinary_highlights": "",
        "must_visit_locations": "",
        "travel_tips": "",
    }

    # Title
    title = content_area.find("h1") or soup.find("h1")
    if title:
        extracted["name"] = _normalize_ws(title.get_text(" ", strip=True))
        print(f"   ‚úì Title: {extracted['name'][:60]}")

    # Overview: ch·ªçn ƒëo·∫°n p d√†i, kh√¥ng r√°c
    paras = [
        _remove_junk(p.get_text(" ", strip=True))
        for p in content_area.find_all("p", limit=12)
    ]
    paras = [p for p in paras if p and len(p) >= 80]
    if paras:
        extracted["overview"] = paras[0]
        print(f"   ‚úì Overview: {len(extracted['overview'])} chars")

    # Map keyword -> field
    def match_field(h_text: str) -> str | None:
        h = (h_text or "").lower()

        for kw in keywords.get("weather", []):
            if kw in h:
                return "weather_and_best_time"
        for kw in keywords.get("transportation", []):
            if kw in h:
                return "transportation"
        for kw in keywords.get("food", []):
            if kw in h:
                return "culinary_highlights"
        for kw in keywords.get("places", []):
            if kw in h:
                return "must_visit_locations"
        for kw in keywords.get("tips", []):
            if kw in h:
                return "travel_tips"
        return None

    headings = content_area.find_all(["h2", "h3", "h4"])

    # gom section theo heading; kh√¥ng set n·∫øu ƒë√£ c√≥ (tr√°nh overwrite)
    for hd in headings:
        field = match_field(hd.get_text(" ", strip=True))
        if not field or extracted.get(field):
            continue

        section_text = _collect_section_text(hd, max_words=260)
        extracted[field] = section_text
        if extracted[field]:
            print(f"   ‚úì {field}: {len(extracted[field])} chars")

    return extracted


async def extract_hub_info(url: str) -> dict:
    """
    WORKFLOW SIMPLIFIED:
    1. Crawl4AI fetch HTML
    2. LLM t√¨m keywords (lightweight)
    3. BeautifulSoup extract content (rule-based)
    4. Metadata helper format JSON
    """

    # ===== B∆Ø·ªöC 1: CRAWL4AI FETCH HTML =====
    print(f"\n{'='*60}")
    print(f"üöÄ B∆Ø·ªöC 1: Crawl4AI fetch HTML")
    print(f"{'='*60}")

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url=url,
            cache_mode=CacheMode.BYPASS
        )

        if not result.success:
            raise Exception(f"Crawl failed: {result.error_message}")

        html_content = result.html
        print(f"‚úÖ ƒê√£ fetch HTML ({len(html_content)} chars)")

    # ===== B∆Ø·ªöC 2: LLM T√åM KEYWORDS =====
    print(f"\n{'='*60}")
    print(f"ü§ñ B∆Ø·ªöC 2: LLM t√¨m heading keywords (lightweight)")
    print(f"{'='*60}")

    keywords = await llm_get_heading_keywords(html_content)
    print(f"‚úÖ Keywords: {json.dumps(keywords, ensure_ascii=False)[:200]}")

    # ===== B∆Ø·ªöC 3: BEAUTIFULSOUP EXTRACT =====
    print(f"\n{'='*60}")
    print(f"üîç B∆Ø·ªöC 3: BeautifulSoup extract content")
    print(f"{'='*60}")

    raw_extracted = extract_content_with_beautifulsoup(html_content, keywords)
    print(f"‚úÖ ƒê√£ extract {len([v for v in raw_extracted.values() if v])} fields c√≥ n·ªôi dung")

    # ===== B∆Ø·ªöC 4: FORMAT JSON FINAL =====
    print(f"\n{'='*60}")
    print(f"üì¶ B∆Ø·ªöC 4: Format JSON final v·ªõi metadata")
    print(f"{'='*60}")

    name = raw_extracted.get("name", "Unknown Destination")
    overview_raw = raw_extracted.get("overview", "")

    # --- chu·∫©n ho√° + gi·ªõi h·∫°n theo y√™u c·∫ßu (word range) ---
    overview = _truncate_words(overview_raw, 120, 250)
    location_geo = _truncate_words(raw_extracted.get("location_and_geography", ""), 80, 150)
    weather = _truncate_words(raw_extracted.get("weather_and_best_time", ""), 120, 200)
    transportation = _truncate_words(raw_extracted.get("transportation", ""), 80, 150)
    locations = _truncate_words(raw_extracted.get("must_visit_locations", ""), 150, 300)
    food = _truncate_words(raw_extracted.get("culinary_highlights", ""), 120, 200)
    tips = _truncate_words(raw_extracted.get("travel_tips", ""), 80, 150)

    # Detect metadata
    content_dict = {
        "overview": overview,
        "weather": weather,
        "locations": locations,
        "food": food,
    }

    metadata = extract_metadata_from_content(name, overview, content_dict)

    print("‚úÖ Metadata detected:")
    print(f"   - Region: {metadata['region']}")
    print(f"   - Province: {metadata['province']}")
    print(f"   - Airport: {metadata['airport']}")
    print(f"   - Climate: {metadata['climate_tag']}")
    print(f"   - Vibe: {metadata['vibe_tag']}")

    # text_content: d√†i h∆°n overview, d√πng cho retrieval
    text_content = _normalize_ws(
        " ".join([
            overview,
            weather[:600],
            locations[:600],
            food[:600],
            tips[:400],
        ])
    )

    hub_id = _hub_id_from_url_or_name(url, name)

    final_data = {
        "id": f"hub_{hub_id}",
        "name": name,
        "metadata": {
            "type": "hub_info",
            "region": metadata["region"],
            "province": metadata["province"],
            "airport": metadata["airport"],
            "climate_tag": metadata["climate_tag"],
            "vibe_tag": metadata["vibe_tag"],
            "sources": [url],
            "last_updated": datetime.now().strftime("%Y-%m-%d"),
        },
        "text_content": text_content,
        "content": {
            "overview": overview,
            "location_and_geography": location_geo,
            "weather_and_best_time": weather,
            "transportation": transportation,
            "must_visit_locations": locations,
            "culinary_highlights": food,
            "travel_tips": tips,
        },
    }

    print("‚úÖ JSON final ho√†n t·∫•t!")
    return final_data


if __name__ == "__main__":
    # Ch·∫°y test nhanh (tu·ª≥ ch·ªçn): b·ªè comment ƒë·ªÉ ch·∫°y.
    import asyncio
    pass

    url = "https://www.dalattrip.com/dulich/du-lich-da-lat-tu-tuc/"
    hub_info = asyncio.run(extract_hub_info(url))
    print(json.dumps(hub_info, indent=2, ensure_ascii=False))
